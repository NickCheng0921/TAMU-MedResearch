{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef97df3-f1a8-4bad-94f7-a41e89efc7ff",
   "metadata": {},
   "source": [
    "Objective of this notebook is to recreate vicreg for pretraining purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4805c3b-b8d3-4745-95ed-b55b6db885ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import average_precision_score as auprc\n",
    "from sklearn.metrics import roc_auc_score as auc_score\n",
    "import keras\n",
    "\n",
    "#from keras.utils import multi_gpu_model\n",
    "from keras.layers import Input, Dense, GRU, Lambda, Permute, Concatenate, Dropout\n",
    "from keras.models import Model\n",
    "from interpolation_layer import single_channel_interp, cross_channel_interp\n",
    "from mimic_preprocessing import load_data, trim_los, fix_input_format\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(10)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ea0ba4-aecc-4f72-b8b5-e50a11b8aa5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhid = 8192\\nmod = tf.keras.Sequential()\\nmod.add(Dense(hid, input_shape=(512,)))\\nmod.add(Dense(hid))\\nmod.add(Dense(hid))\\nmod.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\\nmod.summary()\\n\\nx = tf.random.uniform(shape=(2560, 512))\\ny = tf.random.uniform(shape=(2560, 8192), minval=0, maxval=1, dtype=tf.int32)\\n\\nmod.fit(x, y, epochs=30)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "hid = 8192\n",
    "mod = tf.keras.Sequential()\n",
    "mod.add(Dense(hid, input_shape=(512,)))\n",
    "mod.add(Dense(hid))\n",
    "mod.add(Dense(hid))\n",
    "mod.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "mod.summary()\n",
    "\n",
    "x = tf.random.uniform(shape=(2560, 512))\n",
    "y = tf.random.uniform(shape=(2560, 8192), minval=0, maxval=1, dtype=tf.int32)\n",
    "\n",
    "mod.fit(x, y, epochs=30)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3540b3cd-fea2-4d1f-bdfd-99c80668b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path\n",
    "if '/home/ugrads/n/nickcheng0921/TAMU-MedResearch/' not in sys.path:\n",
    "    sys.path.append('/home/ugrads/n/nickcheng0921/TAMU-MedResearch/')\n",
    "    \n",
    "from helper import hold_out, mean_imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c6daf1-018c-4f29-b987-53e9276e8ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_num = 1\n",
    "epoch = 30\n",
    "hid = 256 #can be 128-512\n",
    "exp_hid = 8192\n",
    "ref_points = 128\n",
    "hours_look_ahead = 48\n",
    "if gpu_num > 0:\n",
    "    batch = 512*gpu_num\n",
    "else:\n",
    "    batch = 512\n",
    "    \n",
    "#vicreg parameters\n",
    "epsilon = .0001 #small scalar to prevent numerical instabilities\n",
    "gamma = 1 #constant target value for SD\n",
    "\n",
    "#nicks notes args\n",
    "vocabulary = 6000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5a116-acfa-4a86-b90d-70ada14cc17e",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "adjust # of patients in mimic_preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63f52da8-7117-4b07-923d-e946ba09df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files ...\n",
      "Loading Done with 12000 patients! Nick\n",
      "10852 10852\n",
      "(10852, 12, 200) 10852\n",
      "X shape: (10852, 48, 200), Y shape: (10852,)\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset - explanation in multivariate notebook\n",
    "vitals, label = load_data(look_ahead_time = hours_look_ahead)\n",
    "vitals, timestamps = trim_los(vitals, hours_look_ahead)\n",
    "x, m, T = fix_input_format(vitals, timestamps)\n",
    "mean_imputation(x, m)\n",
    "x = np.concatenate((x, m, T, hold_out(m)), axis=1)  # input format\n",
    "y = np.array(label)\n",
    "print(f\"X shape: {x.shape}, Y shape: {y.shape}\")\n",
    "timestamp = x.shape[2]\n",
    "num_features = x.shape[1] // 4\n",
    "#     have an array representation.\n",
    "# m : (N, D, tn) where m[i,j,k] = 0 means that x[i,j,k] is not observed.\n",
    "# T : (N, D, tn) represents the actual time stamps of observation;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37766c63-75d8-4862-a78b-9a8619ec126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "patient_notes = pickle.load(open('notes_12000_'+str(hours_look_ahead)+'hrs.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d60fc2-de03-48d0-bab5-bcede2354333",
   "metadata": {},
   "source": [
    "# Unsupervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d4ccca8-ddc3-4d8d-a45e-43a30c147afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_var = 256\n",
    "coef_cov = 1\n",
    "coef_inv = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38b39a6-d27a-4dca-a15d-88fd72add7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customloss(ytrue, ypred):\n",
    "    print(\"AUX loss\")\n",
    "    \"\"\" Autoencoder loss\n",
    "    \"\"\"\n",
    "    # standard deviation of each feature mentioned in paper for MIMIC_III data\n",
    "    wc = np.array([3.33, 23.27, 5.69, 22.45, 14.75, 2.32,\n",
    "                   3.75, 1.0, 98.1, 23.41, 59.32, 1.41])\n",
    "    wc.shape = (1, num_features)\n",
    "    y = ytrue[:, :num_features, :]\n",
    "    m2 = ytrue[:, 3*num_features:4*num_features, :]\n",
    "    m2 = 1 - m2\n",
    "    m1 = ytrue[:, num_features:2*num_features, :]\n",
    "    m = m1*m2\n",
    "    ypred = ypred[:, :num_features, :]\n",
    "    x = (y - ypred)*(y - ypred)\n",
    "    x = x*m\n",
    "    count = tf.reduce_sum(m, axis=2)\n",
    "    count = tf.where(count > 0, count, tf.ones_like(count))\n",
    "    x = tf.reduce_sum(x, axis=2)/count\n",
    "    x = x/(wc**2)  # dividing by standard deviation\n",
    "    x = tf.reduce_sum(x, axis=1)/num_features\n",
    "    return tf.reduce_mean(x)\n",
    "\n",
    "seed = 0\n",
    "results = {}\n",
    "results['inv'] = []\n",
    "results['var'] = []\n",
    "results['cov'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1092dc9-12b5-4c55-8ff2-29f0878f0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LARS from https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/optimizers/lars.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "327fa1dc-1c58-4a40-8379-1bc44e4c55b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model https://github.com/eyalzk/sketch_rnn_keras/blob/002931b9abea957a77382688f37d95afbb2ae6cb/seq2seqVAE.py\n",
    "#https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618\n",
    "class FusionModel(object):\n",
    "    def __init__(self):\n",
    "        if gpu_num > 1:\n",
    "            dev = \"/cpu:0\"\n",
    "        else:\n",
    "            dev = \"/gpu:0\"\n",
    "        with tf.device(dev):\n",
    "            self.main_input = Input(shape=(4*num_features, timestamp), name='input')\n",
    "            self.notes_input = Input(shape=(vocabulary), name='notes_input')\n",
    "            self.notes_output = Dense(hid, activation='sigmoid', name='text_dense_1')(self.notes_input)\n",
    "            self.notes_output = Dropout(0.2)(self.notes_output)\n",
    "            self.notes_output = Dense(hid, activation='sigmoid', name='text_dense_2')(self.notes_output)\n",
    "            \n",
    "            self.sci = single_channel_interp(ref_points, hours_look_ahead)\n",
    "            self.cci = cross_channel_interp()\n",
    "            self.interp = self.cci(self.sci(self.main_input))\n",
    "            self.reconst = self.cci(self.sci(self.main_input, reconstruction=True),\n",
    "                          reconstruction=True)\n",
    "            self.aux_output = Lambda(lambda x: x, name='aux_output')(self.reconst)\n",
    "            self.z = Permute((2, 1))(self.interp)\n",
    "            self.z = GRU(hid, activation='tanh', recurrent_dropout=0.2, dropout=0.2, name='series_output')(self.z)\n",
    "            \n",
    "            self.exp_head = tf.keras.Sequential()\n",
    "            self.exp_head.add(Dense(exp_hid, input_shape=(hid,)))\n",
    "            self.exp_head.add(Dense(exp_hid))\n",
    "            self.exp_head.add(Dense(exp_hid))\n",
    "            \n",
    "            self.notes_output = self.exp_head(self.notes_output)\n",
    "            self.z = self.exp_head(self.z)\n",
    "            \n",
    "            #print(f\"Z SHAPE {z.shape} NOTES SHAPE {notes_input.shape} MERGED SHAPE {merged_input.shape}\")\n",
    "            self.merged_output = Concatenate(name='merged_output')([self.notes_output, self.z])\n",
    "            self.model = Model([self.main_input, self.notes_input], [self.merged_output, self.aux_output])\n",
    "            \n",
    "            trainable_count = np.sum([K.count_params(w) for w in self.model.trainable_weights])\n",
    "            print(f\"train params: {trainable_count}\")\n",
    "        \n",
    "    def calculate_vic_loss(self, y_true, y_pred):\n",
    "        \"\"\"calculate mse across notes and series output for VICReg\"\"\"\n",
    "        branch1, branch2 = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
    "        batch_size = branch1.shape[0]\n",
    "        \n",
    "        #variance\n",
    "        var1, var2 = tf.zeros(shape=[]), tf.zeros(shape=[])\n",
    "        for i in range(hid):\n",
    "            var1 += max(0, gamma - (tf.math.reduce_variance(branch1[:, i], axis=-1) + epsilon)**(1/2))\n",
    "            var2 += max(0, gamma - (tf.math.reduce_variance(branch2[:, i], axis=-1) + epsilon)**(1/2))\n",
    "        var1 /= hid\n",
    "        var2 /= hid\n",
    "        var1 *= coef_var\n",
    "        var2 *= coef_var\n",
    "        #invariance\n",
    "        mse = coef_inv*tf.reduce_sum(K.square(branch1 - branch2))/batch_size\n",
    "        \n",
    "        #covariance\n",
    "        cov1 = tfp.stats.covariance(branch1, sample_axis=1, event_axis=0)\n",
    "        all_cov1 = tf.reduce_sum(K.square(cov1))\n",
    "        diag_cov1 = tf.reduce_sum(K.square(tf.linalg.diag_part(cov1)))\n",
    "        cov1 = coef_cov*(all_cov1 - diag_cov1)/hid\n",
    "        \n",
    "        cov2 = tfp.stats.covariance(branch2, sample_axis=1, event_axis=0)\n",
    "        all_cov2 = tf.reduce_sum(K.square(cov2))\n",
    "        diag_cov2 = tf.reduce_sum(K.square(tf.linalg.diag_part(cov2)))\n",
    "        cov2 = coef_cov*(all_cov2 - diag_cov2)/hid\n",
    "        \n",
    "        results['inv'].append(mse)\n",
    "        results['var'].append(var1+var2)\n",
    "        results['cov'].append(cov1+cov2)\n",
    "        return mse+var1+var2+cov1+cov2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c30ce1-2573-4f55-912b-d60adc29a3d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notes vectorizer\n",
    "Takes input of size n, and vocab of length l.\n",
    "returns vector of (n, l) where l is vocab embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c19293f5-8ec1-46f9-bd92-25035c0ee715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from tensorflow.python.framework.ops import disable_eager_execution, enable_eager_execution\n",
    "#disable_eager_execution()\n",
    "#enable_eager_execution()\n",
    "#custom loss needs eager execution, custom optimizer needs disabled eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b059f4b6-da6e-4c1d-8044-095cce6c9aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import mse\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "680b8772-cd85-4ab6-be38-28d51f501eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold: 1\n",
      "train params: 138167296\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"tf.__operators__.add_5\" \"                 f\"(type TFOpLambda).\n\n{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2]\n\nCall arguments received by layer \"tf.__operators__.add_5\" \"                 f\"(type TFOpLambda):\n  • x=tf.Tensor(shape=(512, 12, 200, 200), dtype=float32)\n  • y=tf.Tensor(shape=(512, 12, 200, 200), dtype=float32)\n  • name=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d5b6f802988a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         verbose=2)\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;31m#model.metrics_names gives loss names for evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/late-fusion2/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/late-fusion2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7207\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7208\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7209\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"tf.__operators__.add_5\" \"                 f\"(type TFOpLambda).\n\n{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2]\n\nCall arguments received by layer \"tf.__operators__.add_5\" \"                 f\"(type TFOpLambda):\n  • x=tf.Tensor(shape=(512, 12, 200, 200), dtype=float32)\n  • y=tf.Tensor(shape=(512, 12, 200, 200), dtype=float32)\n  • name=None"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "notes_vectorizer = TfidfVectorizer(max_features=vocabulary)\n",
    "\n",
    "model = None\n",
    "for train, test in kfold.split(np.zeros(len(y)), y):\n",
    "    print(\"Running Fold:\", i+1)\n",
    "    FM = FusionModel()\n",
    "    model = FM.model  # re-initializing every time\n",
    "    kfold_notes_train = [patient_notes[i] for i in train]\n",
    "    kfold_notes_test = [patient_notes[i] for i in test]\n",
    "    notes_tfidf = notes_vectorizer.fit(kfold_notes_train) #train vocab on train set, then use vectorizer on test set\n",
    "    #https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy\n",
    "    model.compile(\n",
    "        optimizer='adam', run_eagerly = True, #disables graph execution, but allows us to grab tensor values instantly \n",
    "        loss={'merged_output': FM.calculate_vic_loss, 'aux_output': customloss}) #eager needed to view tensors\n",
    "    model.fit(\n",
    "        {'input': x[train], 'notes_input': notes_vectorizer.transform(kfold_notes_train).todense()}, {'merged_output': [], 'aux_output': x[train]},\n",
    "        batch_size=batch,\n",
    "        epochs=epoch,\n",
    "        verbose=2)\n",
    "    #model.metrics_names gives loss names for evaluate\n",
    "    print(model.metrics_names)\n",
    "\n",
    "    i += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3caf48-517e-4018-97c6-38a28ee41816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(results['inv'], label='inv')\n",
    "plt.plot(results['cov'], label='cov')\n",
    "plt.plot(results['var'], label='var')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac897468-7f80-4175-8559-4325faba4186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.title(\"Variance loss across iterations\")\n",
    "plt.plot([i/(2*coef_var) for i in results['var']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a748b5-3b57-404f-83bb-22180ecda8b9",
   "metadata": {},
   "source": [
    "# Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe0d26-5f69-468d-a8fe-aafbba8b693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('reg1_expander.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32405d-c165-4438-b3bd-cac716f58b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
