{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db412f4-3fbb-48d3-84ce-5cdbaeae1881",
   "metadata": {},
   "source": [
    "## TF-IDF notebook\n",
    "Loads data from locally stored notes file and runs TF-IDF\n",
    "\n",
    "The results of the model look ok ~75-80% accuracy, but I'm concerned as to whether or not I should consider other flags given w/ the note data. There's also a description VARCHAR(255) and an iserror flag that I believe I should eventually consider if fine tuning the model. I will also consider using a custom stop word list in the future if the final model uses TF-IDF.\n",
    "\n",
    "Also, I'm unsure of how many noteevents a single patient can have. I'm naively passing the events through the TF-IDF into prediction net layer, but I am unsure of whether or not I should bundle all the notes for a single patient together, in order to give the model as much info as possible when predicting the mortality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f4c97a-0311-4834-8ecf-217eb5afc408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, os, sys, time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ae1dc7-7ac9-4054-a255-90f2451bef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for model\n",
    "num_words = 6000 #vocabulary for TF-IDF, how many words do we want to consider\n",
    "num_files = 10000 #how many patients do we want to load, stick to 500 if speed is a concern\n",
    "test_split = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce4112-e04b-4ba5-9e4c-953cbecea801",
   "metadata": {},
   "source": [
    "## Setup environment variables, and load mortality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a43a83f-d1de-48c9-9852-a70b1bd0eab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PSQL_USER=postgres\n",
      "env: PSQL_PSWD=postgres\n",
      "env: JDBC_PATH=../TAMU-MedResearch/postgresql-42.4.0.jar\n",
      "env: PYSPARK_PYTHON=/home/ugrads/k/kingrc15/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "%env PSQL_USER=postgres\n",
    "%env PSQL_PSWD=postgres\n",
    "%env JDBC_PATH=../TAMU-MedResearch/postgresql-42.4.0.jar\n",
    "%env PYSPARK_PYTHON=/home/ugrads/k/kingrc15/anaconda3/bin/python\n",
    "\n",
    "if \"/home/ugrads/n/nickcheng0921/omop-summary\" not in sys.path:\n",
    "    sys.path.append(\"/home/ugrads/n/nickcheng0921/omop-summary\")\n",
    "import omop_summary\n",
    "\n",
    "sc, session = omop_summary.create_spark_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f80ff7-2ac3-4d5a-8122-be68a8d47fcf",
   "metadata": {},
   "source": [
    "Load note data from local csv\n",
    "\n",
    "Extract mortality labels from mimic3 data by using https://github.com/stmilab/omop-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e101f00-f693-4f47-a2bf-f1102285fba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution 0.02320173184076945 minutes\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#csv is extremely large (3 Gb) or 91,691,299 lines\n",
    "#grab chunks, and predict mortality w/in chunks\n",
    "notes_path = \"../mimic3Notes/physionet.org/files/mimiciii/1.4/NOTEEVENTS.csv\"\n",
    "notes_reader = pandas.read_csv(notes_path, iterator=True)\n",
    "\n",
    "#reader reads from a stream, this means that calling it \"consumes\" the filestream\n",
    "chunk = notes_reader.get_chunk(num_files)\n",
    "notes = [x.TEXT for index, x in chunk.iterrows()]\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Execution {1.0*(end-start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57c74c5d-f0a4-4179-8616-a45663ed3b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution 13.010804132620494 minutes\n"
     ]
    }
   ],
   "source": [
    "m_labels = []\n",
    "start = time.time()\n",
    "for patient in chunk.iterrows():\n",
    "    patient_id = (patient[1].SUBJECT_ID)\n",
    "    m_labels.append(omop_summary.utils.get_table(f\"SELECT * FROM mimiciii.patients WHERE subject_id = {patient_id}\", session).head(1)[0][\"expire_flag\"])\n",
    "end = time.time()\n",
    "print(f\"Execution {1.0*(end-start)/60} minutes\") #1.22 min to load 1000 patients, 6.5 for 5000, 12.5 for 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9451af8-65f2-4994-bd4a-9ffc349281f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewing 10000 records, w/ 52.43% mortality rate\n"
     ]
    }
   ],
   "source": [
    "#ensure that each note has a mortality label\n",
    "assert(len(m_labels) == len(notes))\n",
    "print(f\"Viewing {len(m_labels)} records, w/ {100.0*sum(m_labels)/len(m_labels)}% mortality rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094d45a-b4d3-4a3f-8bf6-b9ea1893cf96",
   "metadata": {},
   "source": [
    "## Fit TF-IDF Vectorizer, and attach linear predictor layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19fbf1f1-d125-43cd-9cdc-443188c042de",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_vectorizer = TfidfVectorizer(max_features=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "188677af-e717-4bad-9b93-ade9297baa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91e0c2cf-aa50-40e2-96fd-18d3b5e96235",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(notes, m_labels, test_size=test_split, random_state=13)\n",
    "\n",
    "notes_tfidf = notes_vectorizer.fit(x_train) #vectorizer needs to have a set of notes fit, before it can transform as it needs the weights\n",
    "embed1 = notes_vectorizer.transform(x_train).todense()\n",
    "embed2 = notes_vectorizer.transform(x_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32c6a218-d9fc-48f3-a2c3-9f32ef7aeca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               768128    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 768,257\n",
      "Trainable params: 768,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#connect to a fully connected NN layer\n",
    "#binary class\n",
    "#https://stackoverflow.com/questions/64764131/how-to-feed-tf-idf-vectorizer-output-as-input-to-a-neural-network-for-classifica\n",
    "\n",
    "#multi class\n",
    "#https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, input_shape = (embed1.shape[1],), activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "    #tf.keras.layers.Dense(5, activation='softmax')])\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f72421e-e580-4300-bf5d-9db3669b3ab0",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13fea14a-d673-4455-a9c1-f97efa7544e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 4s 9ms/step - loss: 0.5355 - accuracy: 0.7381\n",
      "63/63 [==============================] - 1s 7ms/step - loss: 0.4427 - accuracy: 0.7990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44267070293426514, 0.7990000247955322]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(embed1), np.array(y_train))\n",
    "model.evaluate(np.array(embed2), np.array(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
