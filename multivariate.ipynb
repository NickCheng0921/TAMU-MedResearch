{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc64234b-1f73-445a-ae33-645fb0b9eadf",
   "metadata": {},
   "source": [
    "Objective of this notebook is to break down the multivariate interpolation layer from the paper, so that I can run physiological time series through it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "546db274-c7bc-4fbb-be7f-391bd2729303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import average_precision_score as auprc\n",
    "from sklearn.metrics import roc_auc_score as auc_score\n",
    "import keras\n",
    "\n",
    "#from keras.utils import multi_gpu_model\n",
    "from keras.layers import Input, Dense, GRU, Lambda, Permute, Concatenate\n",
    "from keras.models import Model\n",
    "from interpolation_layer import single_channel_interp, cross_channel_interp\n",
    "from mimic_preprocessing import load_data, trim_los, fix_input_format\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ccc26-22af-412f-b6f0-e35ca84f65b9",
   "metadata": {},
   "source": [
    "## Import helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "421ca19a-177c-43a1-816f-041c84b5b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path\n",
    "if '/home/ugrads/n/nickcheng0921/TAMU-MedResearch/' not in sys.path:\n",
    "    sys.path.append('/home/ugrads/n/nickcheng0921/TAMU-MedResearch/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "232a45dc-73d3-433e-9005-8dcbce4ac1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import hold_out, mean_imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fb7e49-39d3-4be4-9d06-a2c0cf441733",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb3c13ee-960f-441e-ae18-2a3aafda240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_num = 1\n",
    "epoch = 1000\n",
    "hid = 256\n",
    "ref_points = 128\n",
    "hours_look_ahead = 48\n",
    "if gpu_num > 0:\n",
    "    batch = 512*gpu_num\n",
    "else:\n",
    "    batch = 512\n",
    "    \n",
    "#nicks notes args\n",
    "vocabulary = 6000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1480606-02e4-41da-a3c7-8956feea83f5",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "adjust # of patients in mimic_preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e32f85c0-7849-4cb5-b715-19a1b092c895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files ...\n",
      "Loading Done with 5000 patients! Nick\n",
      "4532 4532\n",
      "(4532, 12, 200) 4532\n",
      "X shape: (4532, 48, 200), Y shape: (4532,)\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset\n",
    "# y : (N,) discrete for classification, real values for regression\n",
    "# x : (N, D, tn) input multivariate time series data with dimension\n",
    "#     where N is number of data cases, D is the dimension of\n",
    "#     sparse and irregularly sampled time series and tn is the union\n",
    "#     of observed time stamps in all the dimension for a data case n.\n",
    "#     Since each tn is of variable length, we pad them with zeros to# Loading dataset\n",
    "# y : (N,) discrete for classification, real values for regression\n",
    "# x : (N, D, tn) input multivariate time series data with dimension\n",
    "#     where N is number of data cases, D is the dimension of\n",
    "#     sparse and irregularly sampled time series and tn is the union\n",
    "#     of observed time stamps in all the dimension for a data case n.\n",
    "#     Since each tn is of variable length, we pad them with zeros to\n",
    "#     have an array representation.\n",
    "# m : (N, D, tn) where m[i,j,k] = 0 means that x[i,j,k] is not observed.\n",
    "# T : (N, D, tn) represents the actual time stamps of observation;\n",
    "\n",
    "vitals, label = load_data(look_ahead_time = hours_look_ahead)\n",
    "vitals, timestamps = trim_los(vitals, hours_look_ahead)\n",
    "x, m, T = fix_input_format(vitals, timestamps)\n",
    "mean_imputation(x, m)\n",
    "x = np.concatenate((x, m, T, hold_out(m)), axis=1)  # input format\n",
    "y = np.array(label)\n",
    "print(f\"X shape: {x.shape}, Y shape: {y.shape}\")\n",
    "timestamp = x.shape[2]\n",
    "num_features = x.shape[1] // 4\n",
    "#     have an array representation.\n",
    "# m : (N, D, tn) where m[i,j,k] = 0 means that x[i,j,k] is not observed.\n",
    "# T : (N, D, tn) represents the actual time stamps of observation;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f90970be-ab71-464c-8dd7-31a661f0f11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4532\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "patient_notes = pickle.load(open('notes_5000_'+str(hours_look_ahead)+'hrs.p', 'rb'))\n",
    "print(len(patient_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de88ce-278e-4680-882d-4a1637ecb1de",
   "metadata": {},
   "source": [
    "# Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5dea9c8-f4fe-4f89-b1c3-c61f1bfb57e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customloss(ytrue, ypred):\n",
    "    \"\"\" Autoencoder loss\n",
    "    \"\"\"\n",
    "    # standard deviation of each feature mentioned in paper for MIMIC_III data\n",
    "    wc = np.array([3.33, 23.27, 5.69, 22.45, 14.75, 2.32,\n",
    "                   3.75, 1.0, 98.1, 23.41, 59.32, 1.41])\n",
    "    wc.shape = (1, num_features)\n",
    "    y = ytrue[:, :num_features, :]\n",
    "    m2 = ytrue[:, 3*num_features:4*num_features, :]\n",
    "    m2 = 1 - m2\n",
    "    m1 = ytrue[:, num_features:2*num_features, :]\n",
    "    m = m1*m2\n",
    "    ypred = ypred[:, :num_features, :]\n",
    "    x = (y - ypred)*(y - ypred)\n",
    "    x = x*m\n",
    "    count = tf.reduce_sum(m, axis=2)\n",
    "    count = tf.where(count > 0, count, tf.ones_like(count))\n",
    "    x = tf.reduce_sum(x, axis=2)/count\n",
    "    x = x/(wc**2)  # dividing by standard deviation\n",
    "    x = tf.reduce_sum(x, axis=1)/num_features\n",
    "    return tf.reduce_mean(x)\n",
    "\n",
    "seed = 0\n",
    "results = {}\n",
    "results['loss'] = []\n",
    "results['auc'] = []\n",
    "results['acc'] = []\n",
    "results['auprc'] = []\n",
    "\n",
    "# interpolation-prediction network\n",
    "\n",
    "\n",
    "def interp_net():\n",
    "    if gpu_num > 1:\n",
    "        dev = \"/cpu:0\"\n",
    "    else:\n",
    "        dev = \"/gpu:0\"\n",
    "    with tf.device(dev):\n",
    "        main_input = Input(shape=(4*num_features, timestamp), name='input')\n",
    "        notes_input = Input(shape=(vocabulary), name='notes_input')\n",
    "        notes_output = Dense(hid, activation='sigmoid', name='notes_output')(notes_input)\n",
    "        sci = single_channel_interp(ref_points, hours_look_ahead)\n",
    "        cci = cross_channel_interp()\n",
    "        interp = cci(sci(main_input))\n",
    "        reconst = cci(sci(main_input, reconstruction=True),\n",
    "                      reconstruction=True)\n",
    "        aux_output = Lambda(lambda x: x, name='aux_output')(reconst)\n",
    "        z = Permute((2, 1))(interp)\n",
    "        z = GRU(hid, activation='tanh', recurrent_dropout=0.2, dropout=0.2)(z)\n",
    "        merged_input = Concatenate()([notes_output, z])\n",
    "        #print(f\"Z SHAPE {z.shape} NOTES SHAPE {notes_input.shape} MERGED SHAPE {merged_input.shape}\")\n",
    "        main_output = Dense(1, activation='sigmoid', name='main_output')(merged_input)\n",
    "        orig_model = Model([main_input, notes_input], [main_output, aux_output])\n",
    "    if gpu_num > 1:\n",
    "        model = multi_gpu_model(orig_model, gpus=gpu_num)\n",
    "    else:\n",
    "        model = orig_model\n",
    "    #print(orig_model.summary())\n",
    "    return model\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0.0000, patience=20, verbose=0)\n",
    "callbacks_list = [earlystop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e8b89e-4156-4568-bd72-a32199bf7f8a",
   "metadata": {},
   "source": [
    "## Notes vectorizer\n",
    "Takes input of size n, and vocab of length l.\n",
    "returns vector of (n, l) where l is vocab embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1afb3647-6e04-4643-aa49-5f0b699babe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b82725d8-e47e-4ced-91af-a68d6732cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab264ea4-cadf-4c8a-8601-14fc6a0ad35f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold: 1\n",
      "6/6 - 7s - loss: 0.7602 - main_output_loss: 0.4262 - aux_output_loss: 0.3341 - main_output_accuracy: 0.8159 - val_loss: 0.5690 - val_main_output_loss: 0.2699 - val_aux_output_loss: 0.2990 - val_main_output_accuracy: 0.9214 - 7s/epoch - 1s/step\n",
      "2/2 [==============================] - 1s 210ms/step\n",
      "{'loss': [0.2939535975456238], 'auc': [0.5745614035087719], 'acc': [0.9162072539329529], 'auprc': [0.1738176088858518]} RECONST LOSS: 0.30518466234207153\n",
      "Running Fold: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6a9496acc12f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         metrics={'main_output': 'accuracy'})\n\u001b[1;32m     16\u001b[0m     model.fit(\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;34m{\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'notes_input'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnotes_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkfold_notes_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'main_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aux_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/late-fusion/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The TF-IDF vectorizer is not fitted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/late-fusion/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/late-fusion/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/late-fusion/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "notes_vectorizer = TfidfVectorizer(max_features=vocabulary)\n",
    "\n",
    "for train, test in kfold.split(np.zeros(len(y)), y):\n",
    "    print(\"Running Fold:\", i+1)\n",
    "    model = interp_net()  # re-initializing every time\n",
    "    kfold_notes_train = [patient_notes[i] for i in train]\n",
    "    kfold_notes_test = [patient_notes[i] for i in test]\n",
    "    notes_tfidf = notes_vectorizer.fit(kfold_notes_train) #train vocab on train set, then use vectorizer on test set\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={'main_output': 'binary_crossentropy', 'aux_output': customloss},\n",
    "        loss_weights={'main_output': 1., 'aux_output': 1.},\n",
    "        metrics={'main_output': 'accuracy'})\n",
    "    model.fit(\n",
    "        {'input': x[train], 'notes_input': notes_vectorizer.transform(kfold_notes_train).todense()}, {'main_output': y[train], 'aux_output': x[train]},\n",
    "        batch_size=batch,\n",
    "        callbacks=callbacks_list,\n",
    "        epochs=epoch,\n",
    "        validation_split=0.20,\n",
    "        verbose=2)\n",
    "    y_pred = model.predict({'input': x[test], 'notes_input': notes_vectorizer.transform(kfold_notes_test).todense()}, batch_size=batch)\n",
    "    y_pred = y_pred[0]\n",
    "    total_loss, score, reconst_loss, acc = model.evaluate(\n",
    "        {'input': x[test], 'notes_input': notes_vectorizer.transform(kfold_notes_test).todense()}, {'main_output': y[test], 'aux_output': x[test]},\n",
    "        batch_size=batch,\n",
    "        verbose=0)\n",
    "    results['loss'].append(score)\n",
    "    results['acc'].append(acc)\n",
    "    results['auc'].append(auc_score(y[test], y_pred))\n",
    "    results['auprc'].append(auprc(y[test], y_pred))\n",
    "    print(results, \"RECONST LOSS:\", reconst_loss)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee958774-8ad5-482d-8b3f-e6470b94be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_look_ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1840e7c-14c4-426c-96a8-fd5d5351a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output of network on 5000 patients\n",
    "lah_arr = [6, 12, 18, 24, 30, 36, 42, 48]\n",
    "acc = [0.9074, 0.9125, 0.9061, 0.9135, 0.9045, 0.9153, 0.9123, .9126]\n",
    "auc = [0.7194, 0.8047, 0.8186, 0.8336, 0.8433, 0.8322, 0.8290, .8426]\n",
    "loss =[0.3132, 0.2617, 0.2673, 0.2380, 0.2434, 0.2531, 0.2502, .22944]\n",
    "auprc = [0, 0, 0, 0, 0, .3488, 0.3076, .3551]\n",
    "\n",
    "paper_auc = [.8027, .8161, .8138, .8256, .8324, .8320, .8376, .8453]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc52c7d-39ff-4d36-ac12-d48d183e0c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Output of fusion model on varying admission times for ~5000 patients')\n",
    "plt.plot(lah_arr, acc, label='acc')\n",
    "plt.plot(lah_arr, auc, label='auc')\n",
    "plt.plot(lah_arr, loss, label='loss')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Notes & time series data used in hrs')\n",
    "plt.savefig('baseline.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5d4053-7f21-4046-b971-0aa098ed86c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Comparison of my model vs baseline from paper')\n",
    "plt.plot(lah_arr, auc, label='mine')\n",
    "plt.plot(lah_arr, paper_auc, label='paper')\n",
    "plt.xlabel('Notes & time series data used in hrs')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c873d4-1d3d-4b2b-b57f-e9d212d26688",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.mean(results['acc']), np.mean(results['auc']), np.mean(results['loss']), np.mean(results['auprc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e23936-a188-45c0-9ace-bd80fb16f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results['acc'], label='acc')\n",
    "plt.plot(results['auc'], label='auc')\n",
    "plt.plot(results['loss'], label='loss')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(f\"Outputs of interpolation network on 5000 patients\")\n",
    "plt.xlabel(\"Run from 1-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858f1a5-a6cb-48ad-8135-5689a16c5142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
